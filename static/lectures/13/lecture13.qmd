---
title: "Lecture 13: Generalizations"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../../assets/stat20.scss"
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: hex-background.png
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

# First Things First

## First Things First

-   What to expect from lab today and tomorrow:

    -   Wednesday: Question 2 parts a-f, Question 1 Workshop
    
    -   Thursday: Question 2 parts g-k, Question 1-2 Workshop
    
. . . 
    
## First Things First

-   Extra Credit assignment details now available on Ed

    -   R Cheatsheet/Reference Guide
    
    -   Due **Thursday, August 11 at 11:59pm** on Gradescope
    
. . . 

# Recap

## Confidence Intervals

-   **Confidence intervals** are an inference procedure which outputs a range of values which we expect the population parameter $\theta$ to reside in.

    -   They have an *upper bound* and a *lower bound*

    -   They are centered around a *point estimate*, which is generally the sample statistic $\hat{\theta}$ you are using to estimate the population parameter

-   Our confidence that the procedure will work (meaning the parameter will lie inside of our interval) is expressed as a percentage $1−α$.

    -   You can call this the *confidence level*.

. . .

## Interpreting a Confidence Interval

-   Confidence interval interpretations are examples of *generalizations*. They are also statements about our confidence in the procedure which gives us the interval to capture the parameter.

- One common interpretation:

. . . 

> We are $1-\alpha$ percent confident that the population parameter lies between *lower bound* and *upper bound*.

. . .

**Note:** This interpretation can (and should) be filled in with details about the context of the problem.

. . . 

## Obtaining a Confidence Interval

-   The main goal: obtaining a *sampling distribution* of our sample statistic.

    -   Once this is done, we can determine which range of values we want the confidence interval to include
    

. . . 

# Types of Confidence Intervals we have discussed

## The Bootstrapped Percentile Confidence Interval

-   **How we obtain the sampling distribution**:

    1. Take bootstrap samples of our original sample and calculate a sample statistic for each of these bootstrap samples
    
    2. Create a dotplot / histogram with these calculated statistics
    
-   **How we obtain the confidence interval**:

    1. Decide our confidence level $1-\alpha$
    
    2. Take the *lower bound* to be the $\frac{\alpha}{2}$th percentile of the bootstrap sample statistics 
    
    3. Take the *upper bound* to be the $1- \frac{\alpha}{2}$th percentile of the bootstrap sample statistics
    
. . . 
    
## The Binomial Interval (for one proportion)

-   **How we obtain the sampling distribution**:

    -   We note that the sample proportion $\hat{p}$ can be written as $Y/n$, where $Y$ is the number of "successes" (1s) in $n$ trials (in a sample size of $n$). Thus, $Y$ is distributed $Binomial(n,p)$
    
    -   Then we take the sampling distribution to be the $Binomial(n,p)$ distribution
    
. . .

## The Binomial Interval (for one proportion)
    
-   **How we obtain the confidence interval**:

    1. Decide our confidence level $1-\alpha$
    
    2. Take the *lower bound* to be value $Y = y_{l}$ that cuts $\frac{\alpha}{2}$ percent of the probability on the left
    
    3. Take the *upper bound* to be the value $Y = y_{u}$ that cuts $\frac{\alpha}{2}$ percent of the probability on the right
    
    4. Divide this upper and lower bound by $n$

. . . 

## Interval using the Normal approximation

-   **How we obtain the sampling distribution**:

    1. Determine whether:
    
        -   The observations in our sample are independent.
        
        -   Our sample size is large enough (dependent on the type of parameter \theta we are trying to estimate).
        
    2. If these conditions hold, then the **Central Limit Theorem** tells us that the sampling distribution of our sample statistic is approximately Normal.
    
. . . 

## Interval using the Normal approximation
    
-   **How we obtain the (95 percent) confidence interval**:

    1. Invoke the *empirical rule* (or *68-95-99.7 rule*). This rule states that for a Normal distribution, 95 percent of the data lies within two standard deviations (errors) of the mean.
    
    2. Take the interval to be $(\hat{\theta} - 2*SE, \hat{\theta} + 2*SE)$,
    where $SE$ (standard error) is given by a formula we can calculate.

. . . 

## Interval using the Normal approximation

-   Yesterday, we derived that the standard error of the sample proportion is given by $\sqrt{\frac{p(1-p)}{n}}$. 

-   Note however, that *we do not know $p$*! So as it stands, that formula cannot be used directly.

-   Our best course of action is to replace $p$ with $\hat{p}$ itself, as it is the best guess that we have for $p$. 

. . . 

## Exercises

-   Consult the *Lecture Code and Supplements Thread* on Ed for today's exercises.

. . . 

# Break

# Hypothesis Testing

-   To illustrate this topic, we will revisit the *sex discrimination* study that we performed an exercise on in a previous lecture.

. . . 

## The Experiment

:::: columns
::: {.column width="50%"}
```{r out.width=600, echo = FALSE, fig.align='center'}
knitr::include_graphics("images/equal-pay.jpg")
```
:::

::: {.column width="50%"}
> Question: Are females unfairly discriminated against in promotion decisions?

- 48 male supervisors given the same personnel file.
- Files were randomly assigned to the supervisors.
- Asked: promote or not.
- Files were identical except gender.
:::
::::


## The Data

. . .

Gender | promote | nopromote 
-------|---------|-----------
Male   |    21   |    3      
Female |    14   |    10    

. . .

$$\textrm{Prop. males that are promoted} = 21/24 = 0.875 \\
\textrm{Prop. females that are promoted} = 14/24 = 0.583$$

. . .

At a first glance, does there appear to be a relationship between promotion and gender?

. . . 

## Structure of Hypothesis Test

There are two competing claims.

:::: columns
::: {.column width="47%"}
[Null Hypothesis, $H_0$]{.inversebox}

"There is nothing going on".

- Promotion and gender are independent.
- No gender discrimination.
- Observed difference in proportions is simply due to chance.
:::

::: {.column width="6%"}
:::

::: {.column width="47%"}
[Alternative Hypothesis, $H_A$]{.inversebox}

"There is something going on.”

- Promotion and gender are dependent.
- There is gender discrimination.
- Observed difference in proportions is not due to chance.
:::
::::

. . . 

## Trial as a Hypothesis test

```{r out.width = 350, echo = FALSE, fig.align='center'}
knitr::include_graphics("images/trial.png")
```
  
**$H_0$**  : Defendant is innocent  vs.  **$H_A$**  : Defendant is guilty

- Present evidence / collect data.
- Judge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?"
- If they were very unlikely to have occurred, then the evidence raises more than a reasonable doubt in our minds about the null hypothesis.


## If the evidence is weak

- If the evidence is not strong enough to reject the assumption of innocence, the jury returns with a verdict of *not guilty*.
    - The jury does not say that the defendant is innocent, just that there is not enough evidence to convict.
- Said statistically: *we fail to reject the null hypothesis*, or *the data is consistent with our model*.

. . .

We never *accept the null hypothesis*.

> Why not?


## Why don't we accept $H_0$?

. . .

The hypothesis test gives us:

$$ P(\textrm{data}\,|\,\textrm{H}_0) $$

. . .

It doesn't give us:

$$ P(\textrm{H}_0\,|\,\textrm{data}) $$

. . .

These are not the same thing.

. . .

$$ P(\textrm{Can't see the sun}\,|\,\textrm{Raining}) \ne P(\textrm{Raining}\,|\,\textrm{Can't see the sun}) $$


## Structure of a Hypothesis Test

1. Start with a **null hypothesis** ( $H_0$ ) that represents the status quo and an **alternative hypothesis** ( $H_A$ ) that represents our research question, i.e. what we're testing for.

2. Calculate the observed **test statistic**: an encapsulation of the evidence.

3. Find the **null distribution** of the test statistic under the assumption that $H_0$ is true, either via simulation (our focus) or mathematical methods.

4. If the test results suggest that the data is consistent with $H_0$, we stick with the $H_0$. If they are inconsistent, then we reject the $H_0$ in favor of $H_A$.


## Gender discrimination: $H_0$ test

What is the *null hypothesis*?

- $H_0$: There is no gender discrimination
- $H_0$: Gender and promotion are independent

. . .

What is the *alternative hypothesis*?

- $H_A$: There is gender discrimination
- $H_A$: Gender and promotion are not independent

. . .

What is our *test statistic*?

- $d = \hat{p}_{M} - \hat{p}_{F}$


## Recall: the data

Gender | promote | nopromote 
-------|---------|-----------
Male   |    21   |    3      
Female |    14   |    10    


<br>
<br>

We can compute our observed test statistic:

$$d_{obs} = \hat{p}_{M} - \hat{p}_{F} d_{obs} = 21/24 - 14/24 = .29$$
. . . 

## Creating a simulation

```{r out.width=750, echo = FALSE, fig.align='center'}
knitr::include_graphics("images/gender-cards1.png")
```

**Face cards**: 13 files not promoted  
**Number cards**: 35 files promoted


## Constructing the Null Distribution

1. Shuffle the deck and deal into two piles of twenty four.
2. This mimics the process of each supervision being randomly assigned a male or female file.
3. Compute the proportion that is promoted in each:

. . .

$$d = \hat{p}_{M} - \hat{p}_{F}$$

> Repeat steps 1-3 and store each one.


## {}

```{r out.width=750, echo = FALSE, fig.align='center'}
knitr::include_graphics("images/gender-cards2.png")
```

. . .

> Instead of shuffling cards, we'll shuffle data frames using R...


## Simulation using `infer`

```{r simgender, fig.heigh = 5, echo = FALSE, message = FALSE, fig.align='center'}
library(tidyverse)
gender <- rep(c("M", "F"), each = 24)
promote <- rep(c("Yes", "No"), c(35, 13))
paygap <- tibble(gender, promote)
n_reps <- 5000

library(infer)
null <- paygap %>%
  specify(response = promote,
          explanatory = gender,
          success = "Yes") %>%
  hypothesize(null = "independence") %>%
  generate(reps = n_reps, type = "permute") %>%
  calculate(stat = "diff in props",
            order = c("M", "F"))

p1 <- null %>%
  visualize() +
  shade_pvalue(obs_stat = .29, direction = NULL)
p1
```

. . .

> Do the results of the simulation you just ran provide convincing evidence of gender discrimination against women, i.e. dependence between gender and promotion decisions?


## Computing a p-value

A **p-value** is the probability of observing the observed statistic or more extreme if $H_0$ is true.

. . .

In a simulation setting . . . a p-value is the _proportion_ of test statistics simulated assuming $H_0$ is true that are more extreme than the observed test statistic.

. . .

:::: columns
::: {.column width="50%"}
```{r fig.height=5, echo = FALSE}
p1
```
:::

::: {.column width="50%"}
Of the `r n_reps` simulated test statistics, `r sum(abs(null$stat) > .29)` of them were more extreme than the observed value of `r .29`.

> P-value: `r sum(abs(null$stat) > .29)/n_reps`.

:::
::::

## The p-value and $\alpha$

- The purpose of the p-value is to compare it in relation to a significance level $\alpha$ which is between 0 and 1. This $\alpha$ is not entirely unrelated to the one we mentioned when defining confidence level.


- We *reject* the null hypothesis when

. . . 

$$\textrm{p-value} < \alpha$$

- **Note:** $\alpha$ should be set *before seeing the data*

- The most common value of $\alpha$ is $.05$, though others can be used depending on your situation.

    -   The smaller the $\alpha$, the harder it is to reject your null hypothesis.
    
. . .

# End of Lecture 13





    
