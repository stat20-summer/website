---
title: "Lecture 23: Bonus Material"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../assets/stat20.scss"
    highlight-style: breezedark
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: "images/hex-background.png"
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

# First Things First

## First Things First

-    Exam Grades published
    
-   Quiz 2 and Quiz 2 Retake Grades published

-   Lab grades will be finished by this weekend

. . . 

## First Things First

-   Extra Credit Cheatsheet due **tonight** at 11:59pm

    -   hard deadline

-   Test Corrections due **tomorrow night** at 11:59pm

    -   jeremysanchez@berkeley.edu
    
    -   Will accept submissions 2 hours after
    
. . . 

# The Data Analysis Pipeline

## The Data Analysis Pipeline

![](images/my-rds-data-science-pipeline.png)

::: footer
Source: R for Data Science - Ch 1.1 (Introduction)
:::

# The Statistical Question

## The Statistical Question

-   I placed this step outside of the programming box in Wickham and Grolemund's diagram.

-   Every statistical analysis (modeling or not) starts with some form of question.

## The Statistical Question - a nonmodeling example

![](images/dos.jpg)

## The Statistical Question

![](images/dos.jpg)

. . . 

- A possible *question*: "Is DOS the World's #2 card game?"

- But remember our first exercise of the year?

. . . 

>  What does it mean to be "The World's #2 card game?"

. . . 

- Our initial question is not specific to identify a tangible data set to answer it with. 

## The *Specific* Statistical Question

>  What does it mean to be "The World's #2 card game?"

. . . 

-   You guys had plenty of good answers. If I still had them, I would show them to you. (Rest assured, the paper on which you wrote has most likely been recycled).

-   One more specific answer we came up with was:

. . . 

>  "Is DOS the world's #2 best selling card game?"

. . . 

## Getting the Data

-   We can get data to answer our specific questions from existing sources, or we can collect it ourselves (like you guys did in the La Croix taste testing lab).

. . . 

::: poll
Theorize an example dataset which might help us answer our specific question from the last slide. Identify the unit of observation (a row in the dataset) and give specific variables (columns in the data set) for each unit.
::: 

. . . 

## Importing the Data

-   Wickham & Grolemund (the creators of the image in the last slide)
single out *importing* data in for good reason

    -   It's often not a trivial exercise, particularly if you have lots of data
    
    -   There are many different file formats in which data is stored, too

. . . 

## Importing the Data

-   Throughout this class, we have imported data from:

    -   packages/libraries (example: the `stat20data` library)
    
    -   HTML/webpages (example: the `NBA` dataset used in class)
    
    -   standalone files (example: the 'Restaurants` data from Lab 6)
    
        -   popular file formats include .csv, .json, .txt and .Rda
        
. . . 

## Importing the Data: Examples

-   From a package:

. . . 

```{r, eval = F, echo = TRUE}
library(stat20data)
data(arbuthnot)
```
    
. . . 

-   From a file:

```{r eval = FALSE, echo = TRUE, message = FALSE}
library(tidyverse)
Restaurants <- read_csv("berkeley-restaurants.csv")
```
    
```{r echo = FALSE, message = FALSE}
library(tidyverse)
```
. . . 

# Wrangling Data

## Wrangling Data

-   To Wickham & Grolemund, *tidying* data means having each row correspond to the unit of observation that relates to your statistical question, with columns serving as variables.

    -   This is not always immediately the case.
    
-   When we create new variables out of existing ones, we are *transforming* our data.

-   Together, *tidying* and *transforming* our data is what Wickham & Grolemund define as **data wrangling**.

. . . 

## Wrangling Data - Example

-   Many of you guys may remember me using `as.numeric()` all over the variables in the `NBA` dataset from Weeks 2 and 3. Why did I do that?

. . .

```{r, echo = FALSE,message = FALSE}
library(rvest)

url <- "https://www.basketball-reference.com/leagues/NBA_2022_per_game.html#per_game_stats"

NBA <- (read_html(url) %>% html_table())[[1]]  %>%
  filter(Tm != "TOT")
```

```{r, echo = TRUE}
NBA %>% summarise(Mean_Age = mean(Age))
```

-   What? Shouldn't Age be numeric? Well, it should be . . .

. . . 

## Wrangling Data - Example

We can use the base R function `typeof()` inside of `summarise()` to figure out the data type of the `Age` column.

. . . 

```{r, echo = TRUE}
NBA %>% summarise(Type = typeof(Age))
```

. . . 

-   `Age` is NOT being treated numerically as it stands.

. . . 

## Wrangling Data - Example

-   What is causing the issue?

-   Let's check the different values inside of `Age`.

. . . 

```{r, echo = TRUE}
NBA %>% group_by(Age) %>% summarise(Count = n()) %>% 
  arrange(desc(Age)) %>% head(n = 6)
```

-   `"Age"` is not a number, but there are 30 observations of it!

. . . 

## Wrangling Data - Example

-   Filtering out just the rows with `Age = "Age"`:

```{r, echo = TRUE}
Age_Rows <- NBA %>% filter(Age == "Age")

head(Age_Rows, n = 6)
```

. . . 

-   Ahh. Are there any patterns to these rows occurring?

. . . 

## Wrangling Data - Example

-   At this point we went to the course server and showed how the `View()` function (also accessible by clicking the spreadsheet next to a data frame in the Environment pane of your RStudio session) can be useful here.

. . .


## Wrangling Data - Example

-   First, we need to get rid of the problematic delimiting rows. Their inclusion is causing what should otherwise be a numeric column to be **cast** as a character column.

-   We can convert columns to different data types using `mutate()` and various base R functions. The base function we want here is `as.numeric()`.

. . . 

```{r, echo = TRUE}
#Over-writing the NBA data frame (and thus, the old Age column ) by saving the piped NBA back into itself.

## Dropping NA
NBA <- NBA %>% filter(Tm != "Tm") %>% mutate(Age = as.numeric(Age))
```

. . . 

## Wrangling Data - Example

-   Now performing the calculation we wanted to before: 

. . . 

```{r, echo = TRUE}
NBA %>% summarise(Mean_Age = mean(Age))
```

. . . 

-   That is something like what we were looking to get at the beginning!

. . . 

# Transformation, Visualization, Modeling

## Transformation, Visualization, Modeling

![](images/rds-data-science-pipeline.png)

-   Note in this image that all three of the above steps are in the gray box!

-   They all inform one another and are done circularly.

-   Although often only the transformation and visualization steps are the most talked about in the following label, Wickham and Grolemund label *everything in the box* **Exploratory Data Analysis (EDA)** .
. . . 

::: footer
Source: R for Data Science - Ch 1.1 (Introduction)
:::

## Exploratory Data Analysis (EDA)

-   EDA is *not* just purely transforming your data and creating interesting plots.

    -   EDA *is* using these transformations and interesting plots to:
    
        -   Refine/Redefine your existing questions 
        
        -   Come up with new questions!
    
. . . 

## Using EDA to redefine a question - example

-   When I started what would become my [NFL Point Totals project (linked here)](https://github.com/jeremy-sanchez/Over_49), my initial specific question was:

. . . 

>  Can we predict whether teams will combine for many/few points? 

. . . 

-   However, eventually my question narrowed further to:

. . .

>   Can we predict whether teams will combine for Over/Under 49 points?

. . . 

## Using EDA to redefine a question - example

-   *How did this extra specificity come about?*

    -    I plotted a histogram of the total points scored in games from Weeks 1 through 11 of the 2020 NFL Season
    
    -   I *then noticed* that the mean of the distribution was around 49 points
    
-   From then, I had a more focused question which would then help me choose...

- A model!

. . . 

# Modeling

## Modeling

-   In terms of EDA, we can think of *modeling* as the actionable item that results from refining or coming up with new questions.

-   The most important question to think about is: *what* models should we use? This question can be answered with the visualization/transformation steps in the EDA box.

. . . 

## Modeling - Specific Question

>  Can we predict whether teams will combine for many/few points? 

. . . 

- Possible models (there are more):

    -   Linear Regression
  
    -   $k$-Nearest Neighbors
  
    -   Logistic Regression
  
    -   Tree-based methods (Bagging, Random Forest, etc.)
  
    -   Support Vector Machines

. . . 

## Modeling - Even More Specific Question

>  Can we predict whether teams will combine for Over/Under 49 points? 

. . . 

- Possible models:
  
    -   $k$-Nearest Neighbors
  
    -   Logistic Regression
  
    -   Tree-based methods (Bagging, Random Forest, etc.)
  
    -   Support Vector Machines

. . . 

-   This is now a classifcation problem! (Linear Regression won't do)

. . . 

## Modeling - What is your purpose?

-   This question is particularly important when dealing with Linear Regression methods:

-   Are we?

    -   merely interested in prediction?
    
    -   merely interested in the truth of the linear relationship between our predictors and outcome?
    
    -   interested in both of the above?
    
. . . 

# Inference with Linear Regression

## Inference with Linear Regression

-   In order to do *inference* with Linear Regression (i.e., feel good about our coefficients $b_j$ being *good* estimates of the true coefficients $\beta_j$), we need to make sure some model assumptions are met:

    -   Linearity: linear trend between X and Y

    -   Independent errors: check with residual plot for serial correlation (this is a univariate plot)
    
    -   Errors with constant variance: look for constant spread in residual plot (this is a bivariate plot)
    
    -  Normally distributed residuals: look at histogram of residuals.
    
. . . 

## Inference with Linear Regression

-   We can check these assumptions using *residual plots*.

```{r, message = FALSE, echo = FALSE}
library(tidyverse)
Restaurants <- read_csv("berkeley_restaurants.csv")
lab6model <- lm(log(Reviews) ~ log(Num_Photos), data = Restaurants)
```

-   We will use the linear model from Lab 6, when we tried to predict log photos posted on Google about Berkeley restaurants by log reviews.

. . . 

## Inference with Linear Regression

```{r, message = FALSE, echo = FALSE}
Results_DF <- tibble(Res = residuals(lab6model), 
                     FV = fitted.values(lab6model))

ggplot(Results_DF) + geom_point(mapping = aes(x = FV, y = Res)) +
  xlab("Fitted Values") + ylab("Residuals") + ggtitle("Residuals vs Fitted Values") + theme_light()

```

-   We see (mostly) good things here.

. . . 

## Inference with Linear Regression

```{r, message = FALSE, echo = FALSE}
Results_DF <- tibble(Res = residuals(lab6model), 
                     FV = fitted.values(lab6model),
                     Index = 1:length(residuals(lab6model)))

ggplot(Results_DF) + geom_line(mapping = aes(x = Index, y = Res)) +
  xlab("Index") + ylab("Residuals") +
  ggtitle("Residuals by Index") + theme_light()

```
. . . 

-   Again, mostly good.

. . . 

## Inference with Linear Regression

```{r, message = FALSE, echo = FALSE}
ggplot(Results_DF) + geom_histogram(mapping = aes(x = Res),
                                    color = "white",
                                    binwidth = .1) +
  xlab("Residuals") + ylab("Frequency") +
  ggtitle("Histogram of Residuals") + theme_light()

```

-   Ehh... not so good.

    -   We want this histogram to be normally distributed. This is left skewed.
    
. . . 

## Inference with Linear Regression

-   If we check residual plots and *do* find that our assumptions hold, we can perform inference on our covariates.

    -   Confidence intervals: Using the bootstrap or mathematical methods
    
    -   Hypothesis testing: Using mathematical methods
    
. . . 

## Inference with Linear Regression - Hypothesis Testing

-   On individual predictors:

. . . 

$$H_0: \beta_j = 0$$

$$H_A: \beta_j \neq 0 $$
. . . 

## Inference with Linear Regression - Hypothesis Testing

-   On all of our predictors, together. For $p$ predictors:

$$H_0: \beta_0 = \beta_1  = ... = \beta_p = 0$$

$$H_A: \textrm{At least one } \beta_j \textrm{ is not equal to 0 }  $$
. . . 

# The Data Analysis Pipeline

## Communication

-   The most important step of the data analysis pipeline

    -   What good is your work if you can't communicate it clearly and attractively?
    
-   This is why we have been working in a readable and universal format such as RMarkdown this semester

    -   Section Headers
    
    -   Use of Code Chunk Options
    
. . . 


## Programming

-   Programming underpins all the other steps

    -   We use programming to import data, wrangle data, visualize and model data and to communicate our results
    
. . . 

## The Data Analysis Pipeline

![](images/my-rds-data-science-pipeline.png)

::: footer
Source: R for Data Science - Ch 1.1 (Introduction)
:::


# End of Lecture 23

Thank you all for a fantastic semester! Goodbye and enjoy little time off!

Blessings,

Prof Jeremy

. . . 


