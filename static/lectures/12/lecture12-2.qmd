---
title: "Lecture 12: Generalizations"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../../assets/stat20.scss"
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: hex-background.png
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

# First Things First

## First Things First

-   Midterm scores (re) released

    -   Left skewed distribution; median of 19 (out of 24)

-   Notify me of clobber policy decision by **Sunday, July 24 at 11:59pm**

-   The final assignment will be *an exam* with a "project" element

    -   Wednesday, August 10 during class

    -   This time around, there will be test corrections

    -   If you complete the criterion (TBD) you will receive an addl. 5 percentage points (max score 100 percent).

. . .

## First Things First

-   R Cheatsheet Extra Credit Assignment now available on Gradescope

    -   Due Thursday, August 11

    -   Functions you will need to have on your cheatsheet to receive full credit will located on a soon-to-be-released R Cheatsheet post on Ed.

. . .

## First Things First

-   Lab 3 grades published

-   Lab 2 grades will be published by **Sunday, July 24 at 11:59pm**.

-   PS3 video solutions will be released soon

. . .

# Recap

# Confidence Intervals

## Confidence Intervals

-   **Confidence intervals** are an inference procedure which outputs a range of values which we expect the population parameter to reside in.

    -   They have an *upper bound* and a *lower bound*

    -   They are centered around a *point estimate*, which is generally the sample statistic you are using to estimate the population parameter

-   Our confidence that the procedure will work (meaning the parameter will lie inside of our interval) is expressed as a percentage $1−α$.

    -   You can call this the *confidence level*.

. . .

## Confidence Intervals

-   Example: If $α=.05$, then our confidence that the procedure will work is 1-.05 = .95, which translates to 95 percent.

-   The 95% confidence interval is the most widespread, but it is not a hard and fast rule

    -   You can choose a different number depending on how confident you want to be in your results

. . .

## Interpreting a Confidence Interval

-   Confidence interval interpretations are examples of *generalizations*, hence they are statements about a population parameter which we are trying to estimate.

- One common interpretation:

. . . 

> We are $1-\alpha$ percent confident that the population parameter lies between *lower bound* and *upper bound*.

. . .

**Note:** This interpertation can (and should) be filled in with details about the context of the problem.

. . . 

## Interpreting a Confidence Interval

-   Confidence interval interpretations are not:

    -   statements about the sample statistic (we know everything there is to know about that!)
    
    -   probability statements about observations lying within the bounds of the interval
    
    -   probability statements about the parameter lying within the bounds of the interval (either it does or it doesn't)
    
. . . 

# Confidence Intervals - Computational Method

## Confidence Intervals - Computational Method

> Construct a 1-$α$ confidence interval for your parameter.

. . .

1.  Obtain the *sampling* distribution of the *statistic* which you are using as a *point estimate* for the parameter.

2.  Determine $α$. This will depend on the problem specification (or in the real world, on what you would like it to be).

3.  Obtain the $α$th percentile value of the distribution and the 1-$α$th percentile value. This is because our sampling distribution will be symmetric and we need to take $α$/2 percent from either side to account for the total of $α$ percent.

. . .

## Confidence Intervals

-   There are multiple avenues we can take to construct a confidence interval.

-   Yesterday, we learned about computational methods; today we will examine mathematical methods.

. . .

# Confidence Intervals - Mathematical Methods

## Confidence Intervals - Mathematical Methods

-   Why might we want to use a mathematical method rather than the more intuitive, data-driven (computational) method we have already seen?

    -   Data-driven methods are not always practical

    -   Mathematical methods are widely applicable and easy to employ when they are applicable

# Mathematical Method 1

## Confidence Intervals - Mathematical Methods

-   To start today's lecture, we will head back to the probability concepts of *expected value* and *variance*.

-   Remember the *Bernoulli distribution*? A *Bernoulli* random variable $X$ has the following probability distribution:

| $x$ | $P(X= x)$    |
|-----|--------------|
| 0   | $1-p$        |
| 1   | $p$          |

. . .

## Confidence Intervals - Mathematical Methods

-   If the Bernoulli random variable $X$ records one trial of an event that either results in a 0 or a 1, how can we work with multiple, independent trials of that event?

-   Enter the $Binomial(n,p)$ random variable. This random variable records the sum (number of) successes in $n$ independent trials of an event.

-   We discussed how to calculate Binomial probabilities in detail last week.

. . .

## Confidence Intervals - Mathematical Methods

-   Consider a Binomial random variable $Y$. Under the framework we just discussed, we can write $Y$ mathematically as:

$$ Y = X_1 + X_2 + ... + X_n $$

-   $ X_1, X_2, ..., X_n $ are Bernoulli random variables recording the outcomes of trials $1,2, ..., n$.

. . .

## Confidence Intervals - Mathematical Methods

-   Now, say we have are estimating a parameter (a proportion $p$) with the sample proportion $\hat{p}$ as our *point estimate*.

-   Note we have the relationship $\hat{p} = \frac{Y}{n}$ , where $Y$ is distributed $Binomial(n,p)$

-   Then, to find a confidence interval for $\hat{p}$, we can look at the distribution of $Y$.

. . .

## Confidence Intervals - Mathematical Methods

-   An example: $\hat{p} = 0.5$, $n = 100$. 

-   Then we can write our sample $Y$ as 50. 

```{r, echo = FALSE, message = FALSE}

library(tidyverse)

df <- tibble(y = 0:100,
             p = dbinom(0:100, 100, prob = .5))
ggplot(df, aes(x = y,
               y = p)) +
  geom_col() +
  xlim(qbinom(.001, size = 100, prob = .5), 
       qbinom(.999, size = 100, prob = .5)) +
  labs(title = "Y ~ Binomial(n = 100, p = .5)",
       y = "Probability") +
  theme_light(base_size = 20)

```

. . .

## Confidence Intervals - Mathematical Methods {.smaller}

```{r, echo = FALSE}
df <- tibble(y = 0:100,
             p = dbinom(0:100, 100, prob = .5))
ggplot(df, aes(x = y,
               y = p)) +
  geom_col() +
  xlim(qbinom(.001, size = 100, prob = .5), 
       qbinom(.999, size = 100, prob = .5)) +
  labs(title = "Y ~ Binomial(n = 100, p = .5)",
       y = "Probability") +
  theme_light(base_size = 20)
```
. . .

What values of $Y$ contain the middle 95% of the probability distribution?

-   2.5 percent quantile: 469

-   97.5 percent quantile: 531

. . .

-   Therefore, our 95% CI for $\hat{p}$ is:

    -   $(LB,UB)=(469/1000,531/1000)=(.469,.531)$

. . .

## Confidence Intervals - Mathematical Methods

-   Again, a *Bernoulli* random variable $X$ has the following probability distribution:

. . .

| $x$ | $P(X= x)$    |
|-----|--------------|
| 0   | $1-p$        |
| 1   | $p$          |

. . .

## Confidence Intervals - Mathematical Methods

Exercise:

-   Calculate the variance of a Bernoulli random variable $X$ for a generic $p$ (don't plug any number in for $p$).

. . .

## Confidence Intervals - Mathematical Methods

Exercise:

-   Calculate the variance of the sample proportion $\hat{p} = \frac{Y}{n}$ for a generic $n$.

-   Hint: Write $Y$ in the way we showed earlier in lecture and use the rules for variance of independent random variables we discussed in Week 3.

. . .

## Confidence Intervals - Mathematical Methods

-   In performing the exercises above, we found that the standard deviation of $\hat{p}$ is given by the expression $\sqrt{\frac{p(1-p)}{n}}$.

-   $\hat{p}$ is a *statistic*. Therefore, we give its standard deviation a special name, the **standard error**.

-   This gives us a sense of why the width of a confidence interval might narrow as we increase the sample size $n$.

. . .

## Confidence Intervals - Mathematical Methods

-   So if our sample statistic (*point estimate*) follows a Binomial distribution, and we now have some idea of how it varies, can we create a confidence interval using these two components?

-   The answer is *yes*, and we will see this after the break.

. . .

# Break

## Central Limit Theorem

-   Averages are special quantities in statistics. Proportions are a type of average; an average of 0s and 1s.

-   A *very* important result in statistics, known as the **Central Limit Theorem,** gives the following result, among others (paraphrased from IMS Ch 13):

    > Under certain conditions, the sampling distribution of the sample proportion will be approximated by the Normal distribution.

. . .

## Central Limit Theorem

-   The "certain conditions" referred to in the last slide are:

    -   Observations in the sample must be *independent*.

    -   The sample should be *"large enough"* .

-   There is some dependence on context regarding what the latter condition means specifically.

    -   For inference on single proportions, for example, IMS recommends the *success-failure condition*. This means we need to see at least ten "successes" (1s) and ten "failures" (0s) in our sample.

. . .

## Normal distribution

-   By far the most commonly known probability distribution

-   It is associated with a symmetric, bell-shaped curve.

-   One nice property of the Normal distribution is that its two parameters, $μ$ and $σ$, are *exactly* the distribution's mean and standard deviation, respectively.

. . .

## Normal distribution

-   One nice property about the Normal distribution is the so-called *empirical rule* or *68-95-99.7 rule.*
-   This rule states that:
    -   68 percent of the data lies within *one* standard deviation of the mean

    -   95 percent of the data lies within *two* standard deviations of the mean

    -   99.7 percent of the data lies within *three* standard deviations of the mean

. . .

## Confidence Intervals - Mathematical Methods

-   Therefore, when the two conditions outlined earlier are met, a rough 95 percent confidence interval for the population proportion $p$ can be written as:

. . . 

$$ \hat{p} \pm 2* SE $$ 

-   $\hat{p}$ is the *point estimate*, and $SE$ is its standard error. We found the $SE$ in the single proportion case was $\sqrt{\frac{p(1-p)}{n}}$.

. . .

## Confidence Intervals - Mathematical Methods

-   Similar analogs to the one proportion case exist for when your parameter is one of:
    -   A population mean

    -   A difference in independent population proportions

    -   A difference in independent population means

. . .

# End of Lecture 12
