---
title: "Lecture 22: Prediction Review and Beyond"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../assets/stat20.scss"
    highlight-style: breezedark
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: "images/hex-background.png"
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

# First Things First

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
```

## First Things First

-   Quiz 2 released today at 6:00p

    -   Covers Week 5 material up until Tuesday 
    
    -   25 minutes to complete until tomorrow (Friday) at 5:59p
    
    -   7 questions including **"Don't Cheat"**, just like last time
    
. . . 

## First Things First

-   There will be a retake, similar to last Quiz
    
    -   Retake released Saturday at 6:00p
    
    -   25 minutes to complete until Sunday at 5:59p
    
. . . 

## First Things First

-   Want to drop Lab 6?
    
    -   Notify me by Sunday, 11:59p
    
    -   A reminder on the lab drop/extension policy
    
## First Things First

-   Course Evaluations either available now or soon

    -   Please answer the custom questions! (about how I did as a lecturer, content creator, class policies)
    
. . . 

# Review Exercises

## Review Exercises (45 min.)

-   In the first half of class, we will be working on review exercises attached to the Lecture Code and Supplements post on the Ed.

-   These cover simple and multiple linear regression, as well as $k$-Nearest Neighbors.

. . . 

# Break

# The Data Analysis Pipeline

## The Data Analysis Pipeline

![](images/my-rds-data-science-pipeline.png)

::: footer
Source: R for Data Science - Ch 1.1 (Introduction)
:::

## The Statistical Question

-   I placed this step outside of the programming box in Wickham and Grolemund's diagram.

-   Every statistical analysis (modeling or not) starts with some form of question.

## The Statistical Question - a nonmodeling example

![](images/dos.jpg)

## The Statistical Question

![](images/dos.jpg)

. . . 

- A possible *question*: "Is DOS the World's #2 card game?"

- But remember our first exercise of the year?

. . . 

>  What does it mean to be "The World's #2 card game?"

. . . 

- Our initial question is not specific to identify a tangible data set to answer it with. 

## The *Specific* Statistical Question

>  What does it mean to be "The World's #2 card game?"

. . . 

-   You guys had plenty of good answers. If I still had them, I would show them to you. (Rest assured, the paper on which you wrote has most likely been recycled).

-   One more specific answer we came up with was:

. . . 

>  "Is DOS the world's #2 best selling card game?"

. . . 

## Getting the Data

-   We can get data to answer our specific questions from existing sources, or we can collect it ourselves (like you guys did in the La Croix taste testing lab).

. . . 

::: poll
Theoreize an example dataset which might help us answer our specific question from the last slide. Identify the unit of observation (a row in the dataset) and give specific variables (columns in the data set) for each unit.
::: 

. . . 

## Importing the Data

-   Wickham & Grolemund (the creators of the image in the last slide)
single out *importing* data in for good reason

    -   It's often not a trivial exercise, particularly if you have lots of data
    
    -   There are many different file formats in which data is stored, too

. . . 

## Importing the Data

-   Throughout this class, we have imported data from:

    -   packages/libraries (example: the `stat20data` library)
    
    -   HTML/webpages (example: the `NBA` dataset used in class)
    
    -   standalone files (example: the 'Restaurants` data from Lab 6)
    
        -   popular file formats include .csv, .json, .txt and .Rda
        
. . . 

## Importing the Data: Examples

-   From a package:

. . . 

```{r, eval = F, echo = TRUE}
library(stat20data)
data(arbuthnot)
```
    
. . . 

-   From a file:

```{r, eval = F, echo = TRUE}
library(tidyverse)
Restaurants <- read_csv("berkeley-restaurants.csv")
```
    
. . . 

## Wrangling Data

-   To Wickham & Grolemund, *tidying* data means having each row correspond to the unit of observation that relates to your statistical question, with columns serving as variables.

    -   This is not always immediately the case.
    
-   When we create new variables out of existing ones, we are *transforming* our data.

-   Together, *tidying* and *transforming* our data is what Wickham & Grolemund define as **data wrangling**.

. . . 

## Wrangling Data - Example

-   Many of you guys may remember me using `as.numeric()` all over the variables in the `NBA` dataset from Weeks 2 and 3. Why did I do that?

. . .

```{r, echo = FALSE,message = FALSE}
library(rvest)

url <- "https://www.basketball-reference.com/leagues/NBA_2022_per_game.html#per_game_stats"

NBA <- (read_html(url) %>% html_table())[[1]] %>% 
  filter(Tm != "Tm") %>%
  filter(Tm != "TOT")
```

```{r, echo = TRUE}
NBA %>% summarise(Mean_Age = mean(Age))

```

-   What? Shouldn't Age be numeric? Well, it should be . . .

. . . 

## Wrangling Data - Example


## Transformation, Visualization, Modeling

![](images/rds-data-science-pipeline.png)

-   Note in this image that all three of the above steps are in the gray box!

-   They all inform one another and are done circularly.

-   Although often only the transformation and visualization steps are the most talked about in the following label, Wickham and Grolemund label *everything in the box* **Exploratory Data Analysis (EDA)** .
. . . 

::: footer
Source: R for Data Science - Ch 1.1 (Introduction)
:::

## Exploratory Data Analysis (EDA)

-   EDA is *not* just purely transforming your data and creating interesting plots.

    -   EDA *is* using these transformations and interesting plots to:
    
        -   Refine/Redefine your existing questions 
        
        -   Come up with new questions!
    
. . . 

## Using EDA to redefine a question - example

-   When I started what would become my [NFL Point Totals project (linked here)](https://github.com/jeremy-sanchez/Over_49), my initial specific question was:

. . . 

>  Can we predict whether teams will combine for many/few points? 

. . . 

-   However, eventually my question narrowed further to:

. . .

>   Can we predict whether teams will combine for Over/Under 49 points?

. . . 

## Using EDA to redefine a question - example

-   *How did this extra specificity come about?*

    -    I plotted a histogram of the total points scored in games from Weeks 1 through 11 of the 2020 NFL Season
    
    -   I *then noticed* that the mean of the distribution was around 49 points
    
-   From then, I had a more focused question which would then help me choose...

- A model!

. . . 

## Modeling

-   In terms of EDA, we can think of *modeling* as the actionable item that results from refining or coming up with new questions.

-   The most important question to think about is: *what* models should we use? This question can be answered with the visualization/transformation steps in the EDA box.

. . . 

## Modeling - Specific Question

>  Can we predict whether teams will combine for many/few points? 

. . . 

- Possible models (there are more):

    -   Linear Regression
  
    -   $k$-Nearest Neighbors
  
    -   Logistic Regression
  
    -   Tree-based methods (Bagging, Random Forest, etc.)
  
    -   Support Vector Machines

. . . 

## Modeling - Even More Specific Question

>  Can we predict whether teams will combine for many/few points? 

. . . 

- Possible models:

    -   \sout{Linear Regression}
  
    -   $k$-Nearest Neighbors
  
    -   Logistic Regression
  
    -   Tree-based methods (Bagging, Random Forest, etc.)
  
    -   Support Vector Machines

. . . 

-   This is now a classifcation problem! (Linear Regression won't do)

. . . 

## Modeling - What is your purpose?

-   This question is particularly important when dealing with Linear Regression methods:

-   Are we?

    -   merely interested in prediction?
    
    -   merely interested in the truth of the linear relationship between our predictors and outcome?
    
    -   interested in both of the above?
    
. . . 

# Inference with Linear Regression

## Inference with Linear Regression

-   In order to do *inference* with Linear Regression (i.e., feel good about our coefficients $b_j$ being *good* estimates of the true coefficients $\beta_j$), we need to make sure some model assumptions are met:

    -   Linearity: linear trend between X and Y

    -   Independent errors: check with residual plot for serial correlation (this is a univariate plot)
    
    -   Errors with constant variance: look for constant spread in residual plot (this is a bivariate plot)
    
    -  Normally distributed residuals: look at histogram of residuals.
    
. . . 

## Inference with Linear Regression

-   We can check these with *residual plots*.

-   At this point, we went to the board to do so with the `Restaurants` data-set.

. . . 

## Inference with Linear Regression

-   If we check residual plots and find that our assumptions hold, we can perform inference on our covariates.

    -   Confidence intervals: Using the bootstrap or mathematical methods
    
    -   Hypothesis testing: Using mathematical methods
    
. . . 

## Inference with Linear Regression - Hypothesis Testing

-   On individual predictors:

. . . 

$$H_0: \beta_j = 0$$

$$H_A: \beta_j \neq 0 $$
. . . 

## Inference with Linear Regression - Hypothesis Testing

-   On all of our predictors, together. For $p$ predictors:

$$H_0: \beta_0 = \beta_1  = ... = \beta_p = 0$$

$$H_A: \textrm{At least one } \beta_j \textrm{ is not equal to 0 }  $$
. . . 

# The Data Analysis Pipeline

## Communication

-   The most important step of the data analysis pipeline

    -   What good is your work if you can't communicate it clearly and attractively?
    
-   This is why we have been working in a readable and universal format such as RMarkdown this semester

    -   Section Headers
    
    -   Use of Code Chunk Options
    
. . . 


## Programming

-   Programming underpins all the other steps

    -   We use programming to import data, wrangle data, visualize and model data and to communicate our results
    
. . . 

## The Data Analysis Pipeline

![](images/my-rds-data-science-pipeline.png)

::: footer
Source: R for Data Science - Ch 1.1 (Introduction)
:::

# End of Lecture 22

