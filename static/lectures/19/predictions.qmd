---
title: "Making Predictions"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../assets/stat20.scss"
    highlight-style: breezedark
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: "images/hex-background.png"
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

```{r include = FALSE}
library(tidyverse)
library(patchwork)
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
set.seed(80)
```

## Announcements

- Quiz 4 Again tomorrow 11 am - Friday 11 am
- Midterm walk-through


## {}
```{r echo=FALSE, fig.width=14, fig.height=8, fig.align="center"}
library(ggrepel)
poverty <- read_delim("poverty.txt")
poverty <- poverty %>%
  mutate(hi_grad = case_when(
    State %in% c("New Hampshire", "Minnesota") ~ State, 
    TRUE ~ ""),
    hi_pov = case_when(
      State %in% c("New Mexico", "Arkansas", "Mississippi") ~ State,
      TRUE ~ ""),
    hi_res = case_when(
      State %in% c("Montana", "District of Columbia") ~ State,
      TRUE ~ ""))
p1 <- poverty %>%
  ggplot(aes(x = Poverty, 
             y = Graduates)) +
  geom_point(size = 3) +
  geom_text_repel(aes(label = State),
                  seed = 29) +
  theme_bw(base_size = 18)
p1
```

<!-- We were interested in the relationship between poverty rates and graduates rates, so we started by visualizing it with a scatter plot. We described the strength of the linear relationship with the correlation coefficient. A linear model is more interesting, involving two statistics, so we considered the full range of lines we _could_ fit. -->

<!-- To decide which to use, we need to decide which line is "best" in some sense. -->


## {}

```{r echo=FALSE, fig.width=14, fig.height=8, fig.align="center"}
r <- round(cor(poverty$Graduates,
               poverty$Poverty),
           digits = 2)
p1 +
  annotate("text", x = 17, y = 90, size = 8,
           label = paste0("r = ", r))
```


## {}

```{r echo=FALSE, fig.width=14, fig.height=8, fig.align="center"}
m1 <- lm(Graduates ~ Poverty, data = poverty)
p2 <- p1 +
  annotate("text", x = 17, y = 90, size = 8,
           label = paste0("r = ", r)) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "gray", lwd = 2, alpha = .5) +
  geom_abline(intercept = 100, slope = -1.2, 
              col = "gray", lwd = 2, alpha = .5) +
  geom_abline(intercept = 77, slope = .65, 
              col = "gray", lwd = 2, alpha = .5) +
  geom_abline(intercept = 88, slope = -.6, 
              col = "gray", lwd = 2, alpha = .5) +
  geom_abline(intercept = 84, slope = .01, 
              col = "gray", lwd = 2, alpha = .5)

p2
```


## Ordinary Least Squares

Find the line that minimizes the sum of the squared residuals.

. . .

$$\sum_{i=1}^n (y_i - \hat{y}_i)^2$$

. . .

Two methods of finding that line:

:::: columns
::: {.column width="50%"}
[Numerical Optimization]{.inversebox}
:::
::: {.column width="50%"}
[Analytical Approach]{.inversebox}
:::
::::


## Ordinary Least Squares

$$\sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - \mathbf{b_0} - \mathbf{b_1}x)^2 = f_{RSS}(b_0, b_1)$$

. . .

:::: columns
::: {.column width="50%"}
[Numerical Optimization]{.inversebox}

E.g. Nelder-Mead Algorithm

```{r echo = FALSE, fig.align='center'}
knitr::include_graphics("images/nelder-mead.png")
```
:::
::: {.column width="50%" .fragment}
[Analytical Approach]{.inversebox}

Take partial derivatives of $f_{RSS}$, set to 0, solve.

$$ b_1 = r \frac{s_y}{s_x} $$

$$ b_0 = \bar{y} - b_1 \bar{x} $$
:::
::::


## Ordinary Least Squares

```{r echo=FALSE, fig.width=14, fig.height=8, fig.align="center"}
p2
```


## Ordinary Least Squares

```{r echo=FALSE, fig.width=14, fig.height=8, fig.align="center"}
p2 +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "goldenrod", lwd = 2, alpha = .5)
```


# Estimation in R


## Estimation in R

. . .

```{r fitlm}
m1 <- lm(Graduates ~ Poverty, data = poverty)
summary(m1)
```


## The `lm` object

. . .

```{r showlm}
attributes(m1)
coefficients(m1)
coef(m1)
```


## The `lm` object, cont.

. . .

```{r}
fitted(m1)
```


## The `lm` object, cont.

. . .

```{r}
residuals(m1)
```


# The Linear Model

[What is it good for?]{.smalladage}


## The Linear Model: what is it good for?

1. _Description_: describing the linear relationship between the variables in the data set.

2. _Prediction_: predicting the unknown value of $y_i$ for a new $x_i$ not in the data set.

3. _Residual Analysis_: understanding the deviation of each observation, $y_i$, relative to the model's prediction, $\hat{y}_i$.

## Interpretation of $b_1$

. . .

The **slope** describes the estimated difference in the $y$ variable if the explanatory
variable $x$ for a case happened to be one unit larger.

. . .

```{r}
coef(m1)[2]
```

. . .

*For a state that is one point higher in the percentage of people living below the poverty level, we expect that state's proportion of high school graduates to be is 0.898 lower*.

> Be cautious. If it is observational data, you do not have evidence of a 
*causal link*, but of an association, which still can be used for prediction.


## Interpretation of $b_0$

. . .

The **intercept** is the predicted value that $y$ will take for a case with an $x$ value of zero.

. . .

```{r}
coef(m1)[1]
```

. . .

*For a state that has 0% of people living below the poverty level, we expect that state's proportion of high school graduates to be 96.2%*.

> Do we? While necessary for prediction, the intercept often has no meaningful interpretation.


## The Linear Model: what is it good for?

::: nonincremental
1. _Description_: describing the linear relationship between the variables in the data set.

2. _Prediction_: predicting the unknown value of $y_i$ for a new $x_i$ not in the data set.
:::


## Linear Models for Prediction {.smaller}

:::: columns
::: {.column width="25%"}
::: poll
What graduation rate would you expect for a state with a poverty rate of 15%?

$$\hat{y} = 96.2 - .9 x$$

:::
:::
::: {.column width="75%"}
```{r, echo = FALSE, fig.height = 6, fig.width = 9, fig.align="center"}
p1 + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "goldenrod", lwd = 1.5)
```
:::
::::


## {}

<iframe src="https://embed.polleverywhere.com/multiple_choice_polls/nRKtJXHxf0rfZO3gF0xH6?controls=none&short_poll=true" width="800px" height="600px"></iframe>


## Linear Models for Prediction {.smaller}

:::: columns
::: {.column width="25%"}
::: poll
What graduation rate would you expect for a state with a poverty rate of 15%?

$$\hat{y} = 96.2 - .9 x$$
:::
:::
::: {.column width="75%"}
```{r, echo = FALSE, fig.height = 6, fig.width = 9, fig.align="center"}
p1 + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "goldenrod", lwd = 1.5) +
  geom_vline(xintercept = 15, color = "steelblue", lty = 2, lwd = 1.5)
```
:::
::::


## Linear Models for Prediction {.smaller}

:::: columns
::: {.column width="25%"}
::: poll
What graduation rate would you expect for a state with a poverty rate of 15%?

$$\hat{y} = 96.2 - .9 x$$
:::
:::
::: {.column width="75%"}
```{r, echo = FALSE, fig.height = 6, fig.width = 9, fig.align="center"}
povnew <- data.frame(Poverty = 15)
yhat <- predict(m1, povnew)
p1 + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "goldenrod", lwd = 1.5) +
  geom_vline(xintercept = 15, color = "steelblue", lty = 2, lwd = 1.5) +
  geom_hline(yintercept = yhat, color = "steelblue", lty = 2, lwd = 1.5)
```
:::
::::


## Linear Models for Prediction {.smaller}

:::: columns
::: {.column width="25%"}
::: poll
What graduation rate would you expect for a state with a poverty rate of 15%?

$$\hat{y} = 96.2 - .9 x$$

$$ 96.2 - .9 \times 15 \\
= 82.7$$
:::
:::
::: {.column width="75%"}
```{r, echo = FALSE, fig.height = 6, fig.width = 9, fig.align="center"}
povnew <- data.frame(Poverty = 15)
yhat <- predict(m1, povnew)
p1 + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "goldenrod", lwd = 1.5) +
  geom_vline(xintercept = 15, color = "steelblue", lty = 2, lwd = 1.5) +
  geom_hline(yintercept = yhat, color = "steelblue", lty = 2, lwd = 1.5)
```
:::
::::


## Linear Models for Prediction in R

:::: columns
::: {.column width="47%"}

By "hand":

```{r}
coef(m1)[1] + coef(m1)[2] * 15
```
:::

::: {.column width="6%"}
:::

::: {.column width="47%" .fragment}

Using `predict()`:

```{r}
newx <- tibble(Poverty = 15)
predict(m1, newx)
```

<br>
<br>

>Note: be sure `newx` has the `x` variable named identically to the original df
:::
::::


## How good were the "predictions"?

. . .

::: poll
Which data set(s) will yield a linear model with the best in-sample "predictions"?
:::

. . .

```{r out.width="75%", echo = FALSE, fig.align='center'}
knitr::include_graphics("images/id-the-slr.png")
```

## {}

boardwork

## {background-image="images/r-squared.jpg" background-size="contain"}


## How good were the "predictions"?
 
. . .

$r^2$, the square of the correlation coefficient measures the proportion of total variability in the $y$ that is explained by the linear model.

. . .

- $r^2 \in [0, 1]$

- $r^2$ near 1 means predictions were more accurate.

- $r^2$ near 0 means predictions were less accurate.


## Two classes of predictions

1. _Interpolation_: Prediction using a value of $x$ that is _within_ the range of $x$ values found in the data set used to fit the model.

2. _Extrapolation_: Prediction using a value of $x$ that is _outside_ the range of $x$ values found in the data set used to fit the model.


## Linear Models for Prediction

:::: columns
::: {.column width="25%"}
::: poll
What graduation rate would you expect for a state with a poverty rate of 1%?

$$\hat{y} = 96.2 - .9 x$$
::: 
:::
::: {.column width="75%"}
```{r, echo = FALSE, fig.height = 6, fig.width = 9, fig.align="center"}
povnew <- data.frame(Poverty = 1)
yhat <- predict(m1, povnew)
p1 + 
  xlim(0, 20) +
  ylim(77, 97) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], 
              col = "goldenrod", lwd = 1.5)
```
:::
::::

## {}

```{r echo = FALSE, out.width = "60%"}
knitr::include_graphics("https://imgs.xkcd.com/comics/extrapolating.png")
```

<br>

_Extrapolation is treacherous._

<br>
<br>

<center>
[source: https://xkcd.com/605/]{.footer}
</center>

## The Linear Model: what is it good for?

1. _Description_: describing the linear relationship between the variables in the data set.

2. _Prediction_: predicting the unknown value of $y_i$ for a new $x_i$ not in the data set.

3. _Residual Analysis_: understanding the deviation of each observation, $y_i$, relative to the model's prediction, $\hat{y}_i$.


## Residual Analysis

::: poll
What is the definition of _residual_?
:::

A) $\hat{y}_i - \bar{y}_i$

B) $y_i - \bar{y}_i$

C) $\hat{y}_i - \bar{y}_i$

D) $y_i - \hat{y}_i$

E) $\hat{y}_i - y_i$


## {}

<center>
<iframe src="https://embed.polleverywhere.com/multiple_choice_polls/DmKI9uPZldOiqQeSmRElL?controls=none&short_poll=true" width="800px" height="600px"></iframe>
</center>


## Residual Analysis

. . .

A _residual_ is the difference between the observed value and the "predicted" value of a given $y_i$.

$$ e_i = y_i - \hat{y}_i $$

. . .

Can be used to:

- Describe where a given observation lay relative to the model.
- Uncover more general trends in the interaction of the data and the model.


## Residual Analysis, cont.

Consider the case of Montana.

. . .

```{r echo = FALSE}
p1
```


## Residuals vs $x$

. . .

:::: columns
::: {.column width="43%"}
```{r res1, eval = FALSE}
poverty %>%
  mutate(yhat = fitted(m1),
         res  = residuals(m1)) %>%
  ggplot(aes(x = Poverty,
             y = res)) +
  geom_point(size = 3) +
  geom_text_repel(
    aes(label = State)) +
  theme_bw(base_size = 18)
```
:::
::: {.column width="57%"}
```{r ref.label="res1", echo = FALSE}
```
:::
::::

## Residuals vs $\hat{y}$

:::: columns
::: {.column width="43%"}
```{r res2, eval = FALSE}
poverty %>%
  mutate(yhat = fitted(m1),
         res  = residuals(m1)) %>%
  ggplot(aes(x = yhat,
             y = res)) +
  geom_point(size = 3) +
  geom_text_repel(
    aes(label = State)) +
  theme_bw(base_size = 18)
```
:::
::: {.column width="57%"}
```{r ref.label="res2", echo = FALSE}
```
:::
::::


## Residual plots

What to look for:

- Increasing or descreasing variance in the residuals (_heteroskedasticity_)

- Non-linear trends


## The Linear Model: what is it good for?

1. _Description_: describing the linear relationship between the variables in the data set.

2. _Prediction_: predicting the unknown value of $y_i$ for a new $x_i$ not in the data set.

3. _Residual Analysis_: understanding the deviation of each observation, $y_i$, relative to the model's prediction, $\hat{y}_i$.

