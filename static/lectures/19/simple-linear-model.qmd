---
title: "Lecture 19: Simple Linear Regression"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../assets/stat20.scss"
    highlight-style: breezedark
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: "images/hex-background.png"
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

```{r include = FALSE}
library(tidyverse)
library(ggrepel)
library(patchwork)
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
set.seed(80)
```

# First Things First

## First Things First

-   Quiz 1 grades (including retakes) now fully published

-   Quiz 2 released Thursday at 6:00p

    -   Covers Week 5 material up until tomorrow (Tuesday)
    
    -   There will be a retake, similar to last Quiz
    
. . . 

## First Things First

-   Reminder on Lab Policy


# Prediction

## Prediction

```{r out.height=400, echo = FALSE, fig.align='center', out.width=700}
knitr::include_graphics("images/id-the-slr.png")
```

::: poll
Which **four** plots exhibit the strongest relationships (linear or non-linear)?
:::


## Prediction

```{r out.height=400, echo = FALSE, fig.align='center', out.width=700}
knitr::include_graphics("images/id-the-slr.png")
```

::: poll
Which **two** plots exhibit the strongest *linear* relationships?
:::

## Example: Poverty and Graduation 

- Consider the following question:

. . . 

> What is the relationship between _poverty rate_ and _high school graduation rate_ when looking at the 50 US states?

. . .

## Example: Poverty and Graduation

```{r echo = FALSE, fig.width=8, message = FALSE}
library(tidyverse)
poverty <- read_delim("poverty.txt")
p1 <- ggplot(poverty, aes(x = Poverty, 
                    y = Graduates)) +
  geom_point() +
  theme_bw(base_size = 18)
p1
```

. . . 

-   We have described the relationship with words as **negatively associated**, or as a **negative, linear** trend. 

. . . 

## Example: Poverty and Graduation

-   Is there a way we can capture our description of this relationship with a statistic?

-   There is.

. . . 

# The Correlation Coefficient

## The Correlation Coefficient

-   The **correlation coefficient** $r$ measures the strength of a *linear relationship* between two variables.

-   It is bounded between -1 and 1

    -   When $|r| = 1$, the two variables are related perfectly by a mathematical equation (a line)
    
        -   We call this type of relationship *deterministic*
        
    -   When $r = 0$, we say that there is no *linear relationship* between the two variables.
    
. . . 

## The Correlation Coefficient

    -   The closer in absolute value to 1, the stronger the relationship
    
    -   Negative values of $r$ indicate a negative linear association (negative sloping line)
    
    -   Positive values of $r$ indicate a positivve linear association (positive sloping line)
    
. . . 

## Finding the Correlation Coefficient between two variables in R

```{r echo = FALSE, fig.width=8, message = FALSE}
library(tidyverse)
poverty <- read_delim("poverty.txt")
p1 <- ggplot(poverty, aes(x = Poverty, 
                    y = Graduates)) +
  geom_point() +
  theme_bw(base_size = 18)
p1
```

-   What is the correlation coefficient between these two variables?

. . . 

## Finding the Correlation Coefficient between two variables in R

:::: columns
::: {.column width="60%"}
```{r echo = FALSE, fig.height = 5}
p1
```
:::

::: {.column width="40%" .fragment}

<br>

```{r, echo = TRUE}
poverty %>%
  summarize(r = cor(Graduates, 
                    Poverty))
```
:::
::::

. . .

- This value of $r$ demonstrates once again that the relationship between poverty and graduation is _linear_, _negative_ and _moderately strong_.

## Prediction

-   However, we're no longer just in the Descriptive unit of this course. 

-   Our data seems to follow a line; can we construct a line therefore that might help us *predict* future observations?

. . .

# The Linear Model

## The Linear Model

-   Expresses a predicted value for $y$, $\hat{y}$,  as a linear function of $x$.

. . .

$$\hat{y} = b_0 + b_1x \quad \quad \quad \quad b_0\textrm{: intercept} \quad \quad b_1\textrm{: slope}$$

. . .

```{r echo=FALSE, fig.height = 4.8}
p1 +
  geom_text_repel(aes(label = State)) +
  geom_smooth(method='lm',formula=y~x, se = FALSE)
```


## Residuals

-   A **residual** for observation $i$, $e_i$, is the difference between the observed value $y$ and the predicted value $\hat{y}$.

. . .

$$e_i = y_i - \hat{y}_i$$

. . .

:::: columns
::: {.column width="80%"}
```{r echo=FALSE, fig.height = 4.5}
poverty <- poverty %>%
  mutate(is_MT = case_when(
    State == "Montana" ~ "Montana (13.7, 90.1)", 
    TRUE ~ ""))
m1 <- lm(Graduates ~ Poverty, data = poverty)
mt_pred <- m1$fitted.values[poverty$State == "Montana"]
poverty %>%
  ggplot(aes(x = Poverty, 
             y = Graduates)) +
  annotate("segment", x = 13.7, xend = 13.7, 
           y = mt_pred, yend = 90.1,
           lty = 2,
           color = "tomato", lwd = 1.5) +
  geom_point(size = 3) +
  theme_bw(base_size = 18) +
  geom_text_repel(aes(label = is_MT)) +
  annotate("text", x = 15, y = mt_pred, label = "(13.7, 83.9)") +
  geom_smooth(method='lm',formula=y~x, se = FALSE)
```
:::

::: {.column width="20%" .fragment}
Montana's residual:

$$90.1 - 83.9 \\
= \\
6.2 $$

:::
::::


## Residuals

```{r echo=FALSE, fig.height = 4.8}
p1 +
  geom_text_repel(aes(label = State)) +
  geom_smooth(method='lm',formula=y~x, se = FALSE)
```

::: poll
Which of the following states has the smallest *positive* residual?
:::



# Estimating the coefficients $b_0$ and $b_1$

## Estimation via optimization

<center>
<iframe width="784" height="444" src="https://www.youtube.com/embed/j2gcuRVbwR0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<center>

Nelder-Mead Algorithm (downhill simplex method)


## Estimation via calculus/algebra

See Ed post for derivations of closed form solutions.

The upshot:

. . .

$$ b_1 = \frac{s_y}{s_x}r $$


## Estimating $\beta_1$

```{r find-b1, echo = FALSE, fig.height = 6, fig.width = 10}
p1 <- ggplot(poverty, aes(Poverty, Graduates)) + 
  xlim(0, 20) +
  ylim(75, 96) +
  geom_point(size = 3) +
  theme_bw(base_size = 18)
p1
```

Use $s_x, s_y, \textrm{ and } r$ to calculate $b_1$.


## Estimating $\beta_1$

```{r stats1, eval = FALSE}
stats <- poverty %>%
  summarize(r = cor(Graduates, Poverty),
            sx = sd(Poverty),
            sy = sd(Graduates))
stats
```

. . .

```{r ref.label = "stats1", echo = FALSE}
```

. . .

```{r stats2, eval = FALSE}
stats2 <- stats %>%
  mutate(b1 = sy/sx * r)
stats2
```

. . .

```{r ref.label = "stats2", echo = FALSE}
```


## Estimating $\beta_1$, cont.

```{r find-b1-2, echo = FALSE, fig.height = 6, fig.width = 10}
m1 <- lm(Graduates ~ Poverty, data = poverty)
p2 <- p1 + 
  geom_abline(intercept = 93, slope = m1$coef[2], col = "goldenrod") +
  annotate("text", x = 3, y = 87, label = paste("slope = ", round(m1$coef[2], 2)))
p2
```

Use $s_x, s_y, \textrm{ and } r$ to calculate $b_1$.


## Estimating $\beta_0$

```{r find-b02, echo = FALSE, fig.height = 6, fig.width = 10}
x_bar <- mean(poverty$Poverty)
y_bar <- mean(poverty$Graduates)
p1 + 
  geom_vline(xintercept = mean(poverty$Poverty), 
                col = "steelblue",
                lty = 2) +
  geom_hline(yintercept = mean(poverty$Graduates), 
                col = "steelblue",
                lty = 2) + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], col = "goldenrod") +
  annotate("text", x = 3, y = 91, label = paste("slope = ", round(m1$coef[2], 2))) +
  annotate("text", x = 14, y = 96, label = paste("x-bar = ", round(x_bar, 2))) +
  annotate("text", x = 19, y = 85, label = paste("y-bar = ", round(y_bar, 2)))
```

If the line of best fit *must* pass through $(\bar{x}, \bar{y})$, what is $b_0$?


## Estimating $\beta_0$, cont.

. . .

Since $(11.35, 86.01)$ is on the line, the following relationship holds.

$$ 86.01 = b_0 - 0.9 (11.35) $$

. . .

Then just solve for $b_0$.

$$ b_0 = 86.01 + 0.9 (11.35) = 96.22$$

. . .

More generally:

$$ b_0 = \bar{y} - b_1 \bar{x} $$


## Estimation in R

. . .

```{r fitlm}
m1 <- lm(Graduates ~ Poverty, data = poverty)
```

. . .

```{r}
summary(m1)
```


## The `lm` object {.scrollable}

. . .

```{r showlm}
attributes(m1)
m1$coef
m1$fit
```


## Interpretation of $b_1$

. . .

The **slope** describes the estimated difference in the $y$ variable if the explanatory
variable $x$ for a case happened to be one unit larger.

. . .

```{r}
m1$coef[2]
```

*For each additional percentage point of people living below the poverty level,
we expect a state to have a proportion of high school graduates that is 0.898
lower*.

**Be Cautious**: if it is observational data, you do not have evidence of a 
*causal link*, but of an association, which still can be used for prediction.


## Interpretation of $b_0$

. . .

The **intercept** is the estimated $y$ value that will be taken by a case with 
an $x$ value of zero.

. . .

```{r}
m1$coef[1]
```

While necessary for prediction, the intercept often has no meaningful interpretation.


## Linear Models for Prediction

```{r, echo = FALSE, fig.height = 5, fig.width = 9}
x_bar <- mean(poverty$Poverty)
y_bar <- mean(poverty$Graduates)
p1 + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], col = "goldenrod")
```

$$\hat{y} = 96.2 - .9 x$$


